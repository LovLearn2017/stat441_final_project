{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading google word2vec vectors...\n",
      "word2vec loaded!\n",
      "\n",
      "loading glove word2vec vectors...\n",
      "word2vec loaded!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import sys, re\n",
    "import pandas as pd\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "w2v_file = 'google_word2vec.bin'\n",
    "glove_file = 'glove_word2vec.txt'\n",
    "\n",
    "\n",
    "print( \"loading google word2vec vectors...\" )\n",
    "model1 = KeyedVectors.load_word2vec_format(w2v_file, binary=True)\n",
    "print(  \"word2vec loaded!\\n\" )\n",
    "\n",
    "print( \"loading glove word2vec vectors...\" )\n",
    "model2 = KeyedVectors.load_word2vec_format(glove_file, binary=False)\n",
    "print(  \"word2vec loaded!\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_data( df ):\n",
    "    train = []\n",
    "    dev = []\n",
    "    test = []\n",
    "    vocab = defaultdict(float)\n",
    "\n",
    "    print(df.columns)\n",
    "    for idx, (label, sent, split) in df.iterrows():\n",
    "        words = sent.split()\n",
    "        for word in words:\n",
    "            vocab[word] += 1\n",
    "        datum = { \"y\": label, \"text\": sent }\n",
    "        if split == 'train':\n",
    "            train.append( datum )\n",
    "        elif split == 'dev':\n",
    "            dev.append( datum )\n",
    "        elif split == 'test':\n",
    "            test.append( datum )\n",
    "    return train, dev, test, vocab\n",
    "def get_W(word_vecs, k=300):\n",
    "    \"\"\"\n",
    "    Get word matrix. W[i] is the vector for word indexed by i\n",
    "    \"\"\"\n",
    "    vocab_size = len(word_vecs)\n",
    "    word_idx_map = dict()\n",
    "    W = np.zeros(shape=(vocab_size+1, k), dtype='float32')\n",
    "    W[0] = np.zeros(k, dtype='float32')\n",
    "    i = 1\n",
    "    for word in word_vecs:\n",
    "        W[i] = word_vecs[word]\n",
    "        word_idx_map[word] = i\n",
    "        i += 1\n",
    "    return W, word_idx_map\n",
    "\n",
    "def get_W2(word_vecs1, word_vecs2, k=300):\n",
    "    \"\"\"\n",
    "    Get word matrix. W[i] is the vector for word indexed by i\n",
    "    \"\"\"\n",
    "    vocab_size = len(word_vecs1)\n",
    "    word_idx_map = dict()\n",
    "    W1 = np.zeros(shape=(vocab_size+1, k), dtype='float32')\n",
    "    W2 = np.zeros(shape=(vocab_size+1, k), dtype='float32')\n",
    "\n",
    "    W1[0] = np.zeros(k, dtype='float32')\n",
    "    W2[0] = np.zeros(k, dtype='float32')\n",
    "    i = 1\n",
    "    for word in word_vecs1:\n",
    "        W1[i] = word_vecs1[word]\n",
    "        W2[i] = word_vecs2[word]\n",
    "        word_idx_map[word] = i\n",
    "        i += 1\n",
    "    return W1, W2, word_idx_map\n",
    "\n",
    "def add_unknown_words(word_vecs, vocab, min_df=1, k=300):\n",
    "    \"\"\"\n",
    "    For words that occur in at least min_df documents, create a separate word vector.\n",
    "    0.25 is chosen so the unknown vectors have (approximately) same variance as pre-trained ones\n",
    "    \"\"\"\n",
    "    cnt = 0\n",
    "    for word in vocab:\n",
    "        if word not in word_vecs and vocab[word] >= min_df:\n",
    "            word_vecs[word] = np.random.uniform(-0.25,0.25,k)\n",
    "            cnt += 1\n",
    "    print('missing cnt: ', cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['label', 'sentence', 'split'], dtype='object')\n",
      "Index(['label', 'sentence', 'split'], dtype='object')\n",
      "Index(['label', 'sentence', 'split'], dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:14: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:15: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "google sst1\n",
      "missing cnt:  1626\n",
      "glove sst1\n",
      "missing cnt:  513\n",
      "google sst2\n",
      "missing cnt:  0\n",
      "glove sst2\n",
      "missing cnt:  0\n",
      "google mr\n",
      "missing cnt:  764\n",
      "glove mr\n",
      "missing cnt:  580\n"
     ]
    }
   ],
   "source": [
    "mr_df = pd.read_pickle('{}.pkl'.format('MR'))\n",
    "sst1_df = pd.read_pickle('{}.pkl'.format('SST1'))\n",
    "sst2_df = pd.read_pickle('{}.pkl'.format('SST2'))\n",
    "\n",
    "mr_train, mr_dev, mr_test, mr_vocab = build_data(mr_df)\n",
    "sst1_train, sst1_dev, sst1_test, sst1_vocab = build_data(sst1_df)\n",
    "sst2_train, sst2_dev, sst2_test, sst2_vocab = build_data(sst2_df)\n",
    "\n",
    "combined_vocab = set.union( \n",
    "    set(mr_vocab.keys()), \n",
    "    set(sst1_vocab.keys()), \n",
    "    set(sst2_vocab.keys()) )\n",
    "\n",
    "google_w2v = dict( (w, model1.wv[w]) for w in combined_vocab if w in model1.wv )\n",
    "glove_w2v = dict( (w, model2.wv[w]) for w in combined_vocab if w in model2.wv )\n",
    "\n",
    "print('google sst1')\n",
    "add_unknown_words(google_w2v, sst1_vocab)\n",
    "print('glove sst1')\n",
    "add_unknown_words(glove_w2v, sst1_vocab)\n",
    "\n",
    "print('google sst2')\n",
    "add_unknown_words(google_w2v, sst2_vocab)\n",
    "print('glove sst2')\n",
    "add_unknown_words(glove_w2v, sst2_vocab)\n",
    "\n",
    "print('google mr')\n",
    "add_unknown_words(google_w2v, mr_vocab)\n",
    "print('glove mr')\n",
    "add_unknown_words(glove_w2v, mr_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing cnt:  18891\n"
     ]
    }
   ],
   "source": [
    "google_W, glove_W, word_idx_map = get_W2(google_w2v, glove_w2v)\n",
    "\n",
    "rand_vecs = {}\n",
    "add_unknown_words(rand_vecs, { \n",
    "    k: mr_vocab.get(k, 0) + sst1_vocab.get(k, 0) + sst2_vocab.get(k, 0) \n",
    "    for k in set(mr_vocab) | set(sst1_vocab) | set(sst2_vocab) })\n",
    "random_W, _ = get_W(rand_vecs)\n",
    "\n",
    "assert set(rand_vecs.keys()) == combined_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {\n",
    "    'mr_train': mr_train,\n",
    "    'sst1_train': sst1_train,\n",
    "    'sst1_dev': sst1_dev,\n",
    "    'sst1_test': sst1_test,\n",
    "    'sst2_train': sst2_train,\n",
    "    'sst2_dev': sst2_dev,\n",
    "    'sst2_test': sst2_test,\n",
    "    'google_W': google_W,\n",
    "    'random_W': random_W,\n",
    "    'glove_W': glove_W,\n",
    "    'word_idx_map': word_idx_map,\n",
    "    'vocab': combined_vocab\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(res, open('combined.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
