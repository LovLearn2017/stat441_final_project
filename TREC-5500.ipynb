{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/thushv89/nlp_examples_thushv_dot_com/blob/master/cnn_sentence_classification.ipynb\n",
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "%matplotlib inline\n",
    "from __future__ import print_function\n",
    "import collections\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from matplotlib import pylab\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files found and verified.\n"
     ]
    }
   ],
   "source": [
    "global dir_name,filenames,num_files\n",
    "\n",
    "dir_name = 'question-classif-data'\n",
    "url= 'cogcomp.org/Data/QA/QC/' \n",
    "\n",
    "#url= 'http://cogcomp.org/Data/QA/QC/' \n",
    "\n",
    "if not os.path.exists(dir_name):\n",
    "    os.mkdir(dir_name)\n",
    "\n",
    "filenames = ['trec-train-5500.txt','trec-test.txt']\n",
    "num_files = 2\n",
    "\n",
    "for i in range(len(filenames)):\n",
    "    file_exists = os.path.isfile(os.path.join(dir_name,filenames[i]))\n",
    "    #assert file_exists\n",
    "\n",
    "    if not os.path.isfile(os.path.join(dir_name,filenames[i])):\n",
    "        try:\n",
    "            print('Downloading training and test datasets')\n",
    "            urlretrieve(url+\"train_5500.label\", os.path.join(dir_name,\"trec-train-5500.txt\"))\n",
    "            urlretrieve(url+\"TREC_10.label\", os.path.join(dir_name,\"trec-test.txt\"))\n",
    "            print('Download successful')\n",
    "        except:\n",
    "            print('Something went wrong while downloading data. \\n'+\n",
    "            'Can you manually download data and place them in the question-classif-data folder?\\n'+\n",
    "            'Train: http://cogcomp.org/Data/QA/QC/train_5500.label as trec-train-5500.txt\\n'+\n",
    "            'Test: http://cogcomp.org/Data/QA/QC/TREC_10.label as trec-test.txt')\n",
    "\n",
    "print('Files found and verified.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing file question-classif-data\\trec-train-5500.txt\n",
      "\tQuestion 0: ['manner', 'how', 'did', 'serfdom', 'develop', 'in', 'and', 'then', 'leave', 'russia', '?']\n",
      "\tLabel 0: DESC\n",
      "\n",
      "\tQuestion 1: ['cremat', 'what', 'films', 'featured', 'the', 'character', 'popeye', 'doyle', '?']\n",
      "\tLabel 1: ENTY\n",
      "\n",
      "\tQuestion 2: ['manner', 'how', 'can', 'i', 'find', 'a', 'list', 'of', 'celebrities', \"'\", 'real', 'names', '?']\n",
      "\tLabel 2: DESC\n",
      "\n",
      "\tQuestion 3: ['animal', 'what', 'fowl', 'grabs', 'the', 'spotlight', 'after', 'the', 'chinese', 'year', 'of', 'the', 'monkey', '?']\n",
      "\tLabel 3: ENTY\n",
      "\n",
      "\tQuestion 4: ['exp', 'what', 'is', 'the', 'full', 'form', 'of', '.com', '?']\n",
      "\tLabel 4: ABBR\n",
      "\n",
      "\n",
      "Processing file question-classif-data\\trec-test.txt\n",
      "\tQuestion 0: ['manner', 'how', 'did', 'serfdom', 'develop', 'in', 'and', 'then', 'leave', 'russia', '?']\n",
      "\tLabel 0: DESC\n",
      "\n",
      "\tQuestion 1: ['cremat', 'what', 'films', 'featured', 'the', 'character', 'popeye', 'doyle', '?']\n",
      "\tLabel 1: ENTY\n",
      "\n",
      "\tQuestion 2: ['manner', 'how', 'can', 'i', 'find', 'a', 'list', 'of', 'celebrities', \"'\", 'real', 'names', '?']\n",
      "\tLabel 2: DESC\n",
      "\n",
      "\tQuestion 3: ['animal', 'what', 'fowl', 'grabs', 'the', 'spotlight', 'after', 'the', 'chinese', 'year', 'of', 'the', 'monkey', '?']\n",
      "\tLabel 3: ENTY\n",
      "\n",
      "\tQuestion 4: ['exp', 'what', 'is', 'the', 'full', 'form', 'of', '.com', '?']\n",
      "\tLabel 4: ABBR\n",
      "\n",
      "Max Sentence Length: 38\n",
      "\n",
      "Normalizing all sentences to same length\n",
      "\tTrain questions normalized\n",
      "\tTest questions normalized\n",
      "\t\tSample test question: %s ['dist', 'how', 'far', 'is', 'it', 'from', 'denver', 'to', 'aspen', '?', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD', 'PAD']\n"
     ]
    }
   ],
   "source": [
    "max_sent_length = 0\n",
    "def read_data(filename):\n",
    "  global max_sent_length\n",
    "  questions = []\n",
    "  labels = []\n",
    "  with open(filename,'r',encoding='latin-1') as f:        \n",
    "    for row in f:\n",
    "        row_str = row.split(\":\")\n",
    "        lb,q = row_str[0],row_str[1]\n",
    "        q = q.lower()\n",
    "        labels.append(lb)\n",
    "        questions.append(q.split())        \n",
    "        if len(questions[-1])>max_sent_length:\n",
    "            max_sent_length = len(questions[-1])\n",
    "  return questions,labels\n",
    "\n",
    "\n",
    "global train_questions,train_labels\n",
    "global test_questions,test_labels\n",
    "\n",
    "for i in range(num_files):    \n",
    "    print('\\nProcessing file %s'%os.path.join(dir_name,filenames[i]))\n",
    "    if i==0:\n",
    "        train_questions,train_labels = read_data(os.path.join(dir_name,filenames[i]))\n",
    "        assert len(train_questions)==len(train_labels)\n",
    "    elif i==1:\n",
    "        test_questions,test_labels = read_data(os.path.join(dir_name,filenames[i]))\n",
    "        assert len(test_questions)==len(test_labels)\n",
    "    for j in range(5):\n",
    "        print('\\tQuestion %d: %s' %(j,train_questions[j]))\n",
    "        print('\\tLabel %d: %s\\n'%(j,train_labels[j]))\n",
    "        \n",
    "print('Max Sentence Length: %d'%max_sent_length)\n",
    "print('\\nNormalizing all sentences to same length')\n",
    "\n",
    "for qi,que in enumerate(train_questions):\n",
    "    for _ in range(max_sent_length-len(que)):\n",
    "        que.append('PAD')\n",
    "    assert len(que)==max_sent_length\n",
    "    train_questions[qi] = que\n",
    "print('\\tTrain questions normalized')\n",
    "for qi,que in enumerate(test_questions):\n",
    "    for _ in range(max_sent_length-len(que)):\n",
    "        que.append('PAD')\n",
    "    assert len(que)==max_sent_length\n",
    "    test_questions[qi] = que\n",
    "print('\\tTest questions normalized')  \n",
    "print('\\t\\tSample test question: %s',test_questions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "226176 Words found.\n",
      "Found 8917 words in the vocabulary. \n",
      "All words (count) [('PAD', 161576), ('?', 5790), ('the', 4036), ('what', 3725), ('is', 1959)]\n",
      "0th entry in dictionary: %s PAD\n",
      "\n",
      "Sample data\n",
      "[33, 9, 20, 3622, 2268, 6, 19, 506, 844, 1019, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[37, 3, 845, 630, 2, 161, 1277, 3623, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "Vocabulary size:  8917\n",
      "\n",
      "Train size:  5452\n",
      "Test size:  500\n"
     ]
    }
   ],
   "source": [
    "def build_dataset(questions):\n",
    "    words = []\n",
    "    data_list = []\n",
    "    count = []\n",
    "    for d in questions:\n",
    "        words.extend(d)\n",
    "    print('%d Words found.'%len(words))    \n",
    "    print('Found %d words in the vocabulary. '%len(collections.Counter(words).most_common()))\n",
    "    count.extend(collections.Counter(words).most_common())\n",
    "    dictionary = dict()\n",
    "    for word, _ in count:\n",
    "        dictionary[word] = len(dictionary)\n",
    "        \n",
    "    for d in questions:\n",
    "        data = list()\n",
    "        for word in d:\n",
    "            index = dictionary[word]        \n",
    "            data.append(index)\n",
    "            \n",
    "        data_list.append(data)\n",
    "    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys())) \n",
    "    return data_list, count, dictionary, reverse_dictionary\n",
    "\n",
    "global data_list, count, dictionary, reverse_dictionary\n",
    "\n",
    "all_questions = list(train_questions)\n",
    "all_questions.extend(test_questions)\n",
    "\n",
    "all_question_ind, count, dictionary, reverse_dictionary = build_dataset(all_questions)\n",
    "\n",
    "print('All words (count)', count[:5])\n",
    "print('0th entry in dictionary: %s',reverse_dictionary[0])\n",
    "print('\\nSample data') \n",
    "print(all_question_ind[0])\n",
    "print(all_question_ind[1])\n",
    "\n",
    "print('\\nVocabulary size: ',len(dictionary))\n",
    "vocabulary_size = len(dictionary)\n",
    "\n",
    "print('\\nTrain size: ',len(train_questions))\n",
    "print('Test size: ',len(test_questions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[33,\n",
       " 9,\n",
       " 20,\n",
       " 3622,\n",
       " 2268,\n",
       " 6,\n",
       " 19,\n",
       " 506,\n",
       " 844,\n",
       " 1019,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_question_ind[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'PAD',\n",
       " 1: '?',\n",
       " 2: 'the',\n",
       " 3: 'what',\n",
       " 4: 'is',\n",
       " 5: 'of',\n",
       " 6: 'in',\n",
       " 7: 'a',\n",
       " 8: 'ind',\n",
       " 9: 'how',\n",
       " 10: 'other',\n",
       " 11: \"'s\",\n",
       " 12: 'was',\n",
       " 13: 'who',\n",
       " 14: 'to',\n",
       " 15: ',',\n",
       " 16: 'def',\n",
       " 17: 'are',\n",
       " 18: 'for',\n",
       " 19: 'and',\n",
       " 20: 'did',\n",
       " 21: 'does',\n",
       " 22: 'count',\n",
       " 23: '``',\n",
       " 24: \"''\",\n",
       " 25: 'do',\n",
       " 26: 'name',\n",
       " 27: 'many',\n",
       " 28: 'on',\n",
       " 29: 'desc',\n",
       " 30: 'where',\n",
       " 31: 'country',\n",
       " 32: 'date',\n",
       " 33: 'manner',\n",
       " 34: 'city',\n",
       " 35: 'first',\n",
       " 36: 'when',\n",
       " 37: 'cremat',\n",
       " 38: 'reason',\n",
       " 39: 'i',\n",
       " 40: 'gr',\n",
       " 41: 'you',\n",
       " 42: 'can',\n",
       " 43: 'world',\n",
       " 44: 'state',\n",
       " 45: 'from',\n",
       " 46: 'which',\n",
       " 47: 'animal',\n",
       " 48: 'u.s.',\n",
       " 49: 'that',\n",
       " 50: 'an',\n",
       " 51: 'most',\n",
       " 52: 'by',\n",
       " 53: 'food',\n",
       " 54: 'with',\n",
       " 55: 'as',\n",
       " 56: 'have',\n",
       " 57: 'has',\n",
       " 58: 'why',\n",
       " 59: 'dismed',\n",
       " 60: 'there',\n",
       " 61: 'termeq',\n",
       " 62: 'it',\n",
       " 63: '.',\n",
       " 64: 'color',\n",
       " 65: 'money',\n",
       " 66: 'called',\n",
       " 67: 'year',\n",
       " 68: 'people',\n",
       " 69: 'at',\n",
       " 70: 'were',\n",
       " 71: 'period',\n",
       " 72: 'get',\n",
       " 73: 'be',\n",
       " 74: 'president',\n",
       " 75: 'sport',\n",
       " 76: 'find',\n",
       " 77: 'word',\n",
       " 78: 'his',\n",
       " 79: 'mean',\n",
       " 80: 'american',\n",
       " 81: 'exp',\n",
       " 82: 'two',\n",
       " 83: 'war',\n",
       " 84: 'largest',\n",
       " 85: 'made',\n",
       " 86: 'new',\n",
       " 87: 'much',\n",
       " 88: 'fear',\n",
       " 89: 'between',\n",
       " 90: 'long',\n",
       " 91: \"'\",\n",
       " 92: 'its',\n",
       " 93: 'event',\n",
       " 94: 'product',\n",
       " 95: 'substance',\n",
       " 96: 'used',\n",
       " 97: 'origin',\n",
       " 98: 'company',\n",
       " 99: 'kind',\n",
       " 100: 'movie',\n",
       " 101: 'tv',\n",
       " 102: 'day',\n",
       " 103: 'known',\n",
       " 104: 'about',\n",
       " 105: 'film',\n",
       " 106: 'famous',\n",
       " 107: 'invented',\n",
       " 108: 'dist',\n",
       " 109: 'stand',\n",
       " 110: 'body',\n",
       " 111: 'take',\n",
       " 112: 'one',\n",
       " 113: 'title',\n",
       " 114: 'make',\n",
       " 115: 'live',\n",
       " 116: 'or',\n",
       " 117: 'states',\n",
       " 118: 'john',\n",
       " 119: 'your',\n",
       " 120: 'game',\n",
       " 121: 'all',\n",
       " 122: 'into',\n",
       " 123: 'best',\n",
       " 124: 'book',\n",
       " 125: 'play',\n",
       " 126: 'only',\n",
       " 127: 'up',\n",
       " 128: 'man',\n",
       " 129: 'he',\n",
       " 130: 'time',\n",
       " 131: 'wrote',\n",
       " 132: 'river',\n",
       " 133: 'term',\n",
       " 134: 'come',\n",
       " 135: 'techmeth',\n",
       " 136: 'america',\n",
       " 137: 'old',\n",
       " 138: 'born',\n",
       " 139: 'out',\n",
       " 140: 'their',\n",
       " 141: '`',\n",
       " 142: 'south',\n",
       " 143: 'show',\n",
       " 144: 'last',\n",
       " 145: 'baseball',\n",
       " 146: 'use',\n",
       " 147: 'countries',\n",
       " 148: 'call',\n",
       " 149: 'won',\n",
       " 150: 'my',\n",
       " 151: 'team',\n",
       " 152: 'united',\n",
       " 153: 'star',\n",
       " 154: 'home',\n",
       " 155: 'named',\n",
       " 156: 'mount',\n",
       " 157: 'had',\n",
       " 158: 'not',\n",
       " 159: 'population',\n",
       " 160: 'difference',\n",
       " 161: 'character',\n",
       " 162: 'number',\n",
       " 163: 'if',\n",
       " 164: 'english',\n",
       " 165: 'highest',\n",
       " 166: 'capital',\n",
       " 167: 'died',\n",
       " 168: 'after',\n",
       " 169: 'become',\n",
       " 170: 'water',\n",
       " 171: 'king',\n",
       " 172: 'dog',\n",
       " 173: 'us',\n",
       " 174: 'average',\n",
       " 175: 'veh',\n",
       " 176: 'earth',\n",
       " 177: 'north',\n",
       " 178: 'three',\n",
       " 179: 'perc',\n",
       " 180: 'work',\n",
       " 181: 'four',\n",
       " 182: 'song',\n",
       " 183: 'novel',\n",
       " 184: 'played',\n",
       " 185: 'said',\n",
       " 186: 'information',\n",
       " 187: 'common',\n",
       " 188: 'some',\n",
       " 189: 'die',\n",
       " 190: 'plant',\n",
       " 191: 'actress',\n",
       " 192: 'years',\n",
       " 193: 'space',\n",
       " 194: 'actor',\n",
       " 195: 'mountain',\n",
       " 196: 'black',\n",
       " 197: 'located',\n",
       " 198: 'college',\n",
       " 199: 'group',\n",
       " 200: 'york',\n",
       " 201: 'names',\n",
       " 202: 'during',\n",
       " 203: 'they',\n",
       " 204: 'would',\n",
       " 205: 'say',\n",
       " 206: 'computer',\n",
       " 207: 'will',\n",
       " 208: 'california',\n",
       " 209: 'woman',\n",
       " 210: 'been',\n",
       " 211: 'killed',\n",
       " 212: 'letter',\n",
       " 213: 'national',\n",
       " 214: 'go',\n",
       " 215: 'weight',\n",
       " 216: 'sea',\n",
       " 217: 'island',\n",
       " 218: 'drink',\n",
       " 219: 'part',\n",
       " 220: 'way',\n",
       " 221: 'university',\n",
       " 222: 'top',\n",
       " 223: 'speed',\n",
       " 224: 'than',\n",
       " 225: 'times',\n",
       " 226: 'popular',\n",
       " 227: 'great',\n",
       " 228: 'should',\n",
       " 229: 'line',\n",
       " 230: 'school',\n",
       " 231: 'language',\n",
       " 232: 'longest',\n",
       " 233: 'over',\n",
       " 234: 'history',\n",
       " 235: 'through',\n",
       " 236: 'good',\n",
       " 237: 'big',\n",
       " 238: 'portrayed',\n",
       " 239: 'person',\n",
       " 240: 'more',\n",
       " 241: 'her',\n",
       " 242: 'major',\n",
       " 243: 'red',\n",
       " 244: 'west',\n",
       " 245: 'french',\n",
       " 246: '1',\n",
       " 247: 'symbol',\n",
       " 248: 'makes',\n",
       " 249: 'car',\n",
       " 250: 'cities',\n",
       " 251: 'different',\n",
       " 252: \"'t\",\n",
       " 253: 'life',\n",
       " 254: 'code',\n",
       " 255: 'place',\n",
       " 256: 'horse',\n",
       " 257: 'meaning',\n",
       " 258: 'internet',\n",
       " 259: 'moon',\n",
       " 260: 'battle',\n",
       " 261: 'address',\n",
       " 262: 'leader',\n",
       " 263: 'miles',\n",
       " 264: 'each',\n",
       " 265: 'abbreviation',\n",
       " 266: 'write',\n",
       " 267: 'causes',\n",
       " 268: 'international',\n",
       " 269: 'cost',\n",
       " 270: 'contains',\n",
       " 271: 'created',\n",
       " 272: 'like',\n",
       " 273: 'charles',\n",
       " 274: 'craft',\n",
       " 275: 'colors',\n",
       " 276: 'general',\n",
       " 277: 'became',\n",
       " 278: 'power',\n",
       " 279: 'human',\n",
       " 280: 'feet',\n",
       " 281: 'begin',\n",
       " 282: 'little',\n",
       " 283: 'british',\n",
       " 284: 'russian',\n",
       " 285: 'so',\n",
       " 286: 'built',\n",
       " 287: 'randy',\n",
       " 288: 'kennedy',\n",
       " 289: 'rate',\n",
       " 290: 'airport',\n",
       " 291: 'lang',\n",
       " 292: 'type',\n",
       " 293: 'whose',\n",
       " 294: 'park',\n",
       " 295: 'currency',\n",
       " 296: 'form',\n",
       " 297: 'st.',\n",
       " 298: 'system',\n",
       " 299: 'nickname',\n",
       " 300: 'found',\n",
       " 301: 'features',\n",
       " 302: 'female',\n",
       " 303: 'baby',\n",
       " 304: 'law',\n",
       " 305: 'disease',\n",
       " 306: 'george',\n",
       " 307: 'abb',\n",
       " 308: 'games',\n",
       " 309: 'second',\n",
       " 310: 'queen',\n",
       " 311: 'bridge',\n",
       " 312: 'san',\n",
       " 313: 'love',\n",
       " 314: 'far',\n",
       " 315: 'seven',\n",
       " 316: 'ii',\n",
       " 317: 'real',\n",
       " 318: 'boasts',\n",
       " 319: 'islands',\n",
       " 320: 'house',\n",
       " 321: 'white',\n",
       " 322: 'five',\n",
       " 323: 'following',\n",
       " 324: 'comic',\n",
       " 325: 'blood',\n",
       " 326: 'me',\n",
       " 327: 'air',\n",
       " 328: 'death',\n",
       " 329: 'canada',\n",
       " 330: 'eat',\n",
       " 331: 'james',\n",
       " 332: 'center',\n",
       " 333: 'william',\n",
       " 334: 'children',\n",
       " 335: 'london',\n",
       " 336: 'men',\n",
       " 337: 'england',\n",
       " 338: 'someone',\n",
       " 339: 'office',\n",
       " 340: 'league',\n",
       " 341: 'newspaper',\n",
       " 342: 'european',\n",
       " 343: 'animals',\n",
       " 344: 'bowl',\n",
       " 345: 'japanese',\n",
       " 346: 'biggest',\n",
       " 347: 'washington',\n",
       " 348: 'nn',\n",
       " 349: 'area',\n",
       " 350: 'ocean',\n",
       " 351: 'television',\n",
       " 352: 'hole',\n",
       " 353: 'bill',\n",
       " 354: 'hit',\n",
       " 355: 'soft',\n",
       " 356: 'high',\n",
       " 357: '2',\n",
       " 358: 'nixon',\n",
       " 359: 'germany',\n",
       " 360: 'player',\n",
       " 361: 'letters',\n",
       " 362: 'father',\n",
       " 363: 'oldest',\n",
       " 364: 'series',\n",
       " 365: 'prime',\n",
       " 366: 'minister',\n",
       " 367: 'county',\n",
       " 368: 'business',\n",
       " 369: 'mother',\n",
       " 370: 'member',\n",
       " 371: 'f.',\n",
       " 372: 'radio',\n",
       " 373: 'lawyer',\n",
       " 374: 'hitler',\n",
       " 375: 'married',\n",
       " 376: 'spanish',\n",
       " 377: 'another',\n",
       " 378: 'runs',\n",
       " 379: 'building',\n",
       " 380: 'music',\n",
       " 381: 'words',\n",
       " 382: 'happened',\n",
       " 383: 'chemical',\n",
       " 384: 'store',\n",
       " 385: 'may',\n",
       " 386: 'held',\n",
       " 387: 'definition',\n",
       " 388: 'singing',\n",
       " 389: 'percentage',\n",
       " 390: 'lives',\n",
       " 391: 'africa',\n",
       " 392: 'ship',\n",
       " 393: 'volsize',\n",
       " 394: 'before',\n",
       " 395: 'ice',\n",
       " 396: 'served',\n",
       " 397: '&',\n",
       " 398: 'kentucky',\n",
       " 399: 'temp',\n",
       " 400: 'sun',\n",
       " 401: 'being',\n",
       " 402: 'fast',\n",
       " 403: 'whom',\n",
       " 404: 'know',\n",
       " 405: 'age',\n",
       " 406: 'tax',\n",
       " 407: 'lake',\n",
       " 408: 'christmas',\n",
       " 409: 'travel',\n",
       " 410: 'golf',\n",
       " 411: 'super',\n",
       " 412: 'mississippi',\n",
       " 413: 'cnn',\n",
       " 414: 'point',\n",
       " 415: 'founded',\n",
       " 416: 'sports',\n",
       " 417: 'around',\n",
       " 418: 'department',\n",
       " 419: 'web',\n",
       " 420: 'ball',\n",
       " 421: 'birth',\n",
       " 422: 'role',\n",
       " 423: 'girl',\n",
       " 424: 'texas',\n",
       " 425: 'win',\n",
       " 426: 'prize',\n",
       " 427: 'soviet',\n",
       " 428: 'indians',\n",
       " 429: 'original',\n",
       " 430: 'main',\n",
       " 431: 'australia',\n",
       " 432: 'end',\n",
       " 433: 'greek',\n",
       " 434: 'singer',\n",
       " 435: 'alaska',\n",
       " 436: 'flag',\n",
       " 437: 'tuberculosis',\n",
       " 438: 'tell',\n",
       " 439: 'tree',\n",
       " 440: 'civil',\n",
       " 441: 'gas',\n",
       " 442: 'comedian',\n",
       " 443: 'story',\n",
       " 444: 'football',\n",
       " 445: 'china',\n",
       " 446: 'ever',\n",
       " 447: 'olympic',\n",
       " 448: 'paper',\n",
       " 449: 'head',\n",
       " 450: 'cold',\n",
       " 451: 'hair',\n",
       " 452: 'japan',\n",
       " 453: 'cartoon',\n",
       " 454: 'back',\n",
       " 455: 'pope',\n",
       " 456: 'musical',\n",
       " 457: 'cross',\n",
       " 458: 'discovered',\n",
       " 459: 'indian',\n",
       " 460: 'board',\n",
       " 461: 'street',\n",
       " 462: 'start',\n",
       " 463: 'gold',\n",
       " 464: 'fame',\n",
       " 465: 'instru',\n",
       " 466: 'introduced',\n",
       " 467: '3',\n",
       " 468: 'size',\n",
       " 469: 'saw',\n",
       " 470: 'once',\n",
       " 471: 'card',\n",
       " 472: 'beer',\n",
       " 473: 'soldiers',\n",
       " 474: 'vietnam',\n",
       " 475: 'night',\n",
       " 476: 'bible',\n",
       " 477: 'need',\n",
       " 478: 'worth',\n",
       " 479: 'shot',\n",
       " 480: 'originate',\n",
       " 481: 'boy',\n",
       " 482: 'former',\n",
       " 483: 'light',\n",
       " 484: 'see',\n",
       " 485: 'richard',\n",
       " 486: 'own',\n",
       " 487: 'claim',\n",
       " 488: '5',\n",
       " 489: 'tennis',\n",
       " 490: 'blue',\n",
       " 491: 'century',\n",
       " 492: 'art',\n",
       " 493: 'famed',\n",
       " 494: 'fly',\n",
       " 495: 'mile',\n",
       " 496: 'produce',\n",
       " 497: 'dick',\n",
       " 498: 'appear',\n",
       " 499: 'continent',\n",
       " 500: 'basketball',\n",
       " 501: 'african',\n",
       " 502: 'we',\n",
       " 503: 'marvel',\n",
       " 504: 'planet',\n",
       " 505: 'no',\n",
       " 506: 'then',\n",
       " 507: 'list',\n",
       " 508: 'chinese',\n",
       " 509: 'cards',\n",
       " 510: 'religion',\n",
       " 511: 'family',\n",
       " 512: 'sometimes',\n",
       " 513: 'website',\n",
       " 514: 'six',\n",
       " 515: 'de',\n",
       " 516: 'under',\n",
       " 517: 'los',\n",
       " 518: 'starred',\n",
       " 519: 'species',\n",
       " 520: 'acid',\n",
       " 521: 'instrument',\n",
       " 522: 'tom',\n",
       " 523: 'director',\n",
       " 524: 'prince',\n",
       " 525: 'organization',\n",
       " 526: 'caused',\n",
       " 527: 'tallest',\n",
       " 528: 'flight',\n",
       " 529: '1984',\n",
       " 530: 'berlin',\n",
       " 531: 'latin',\n",
       " 532: 'fought',\n",
       " 533: 'author',\n",
       " 534: 'union',\n",
       " 535: 'jack',\n",
       " 536: 'dubbed',\n",
       " 537: 'went',\n",
       " 538: 'types',\n",
       " 539: 'women',\n",
       " 540: 'steven',\n",
       " 541: 'sioux',\n",
       " 542: 'east',\n",
       " 543: 'temperature',\n",
       " 544: 'down',\n",
       " 545: 'often',\n",
       " 546: 'wine',\n",
       " 547: 'produced',\n",
       " 548: 'americans',\n",
       " 549: 'them',\n",
       " 550: 'while',\n",
       " 551: 'europe',\n",
       " 552: 'look',\n",
       " 553: 'income',\n",
       " 554: 'jackson',\n",
       " 555: 'favorite',\n",
       " 556: '10',\n",
       " 557: 'record',\n",
       " 558: 'strip',\n",
       " 559: 'rock',\n",
       " 560: 'tall',\n",
       " 561: 'every',\n",
       " 562: 'started',\n",
       " 563: 'full',\n",
       " 564: 'boxing',\n",
       " 565: 'monopoly',\n",
       " 566: 'motto',\n",
       " 567: 'beach',\n",
       " 568: 'lived',\n",
       " 569: 'angeles',\n",
       " 570: 'brothers',\n",
       " 571: 'bear',\n",
       " 572: 'element',\n",
       " 573: 'eye',\n",
       " 574: 'desert',\n",
       " 575: 'months',\n",
       " 576: 'elected',\n",
       " 577: 'jaws',\n",
       " 578: 'wall',\n",
       " 579: 'field',\n",
       " 580: 'green',\n",
       " 581: 'eyes',\n",
       " 582: 'band',\n",
       " 583: 'fastest',\n",
       " 584: 'buried',\n",
       " 585: 'el',\n",
       " 586: 'peter',\n",
       " 587: 'million',\n",
       " 588: 'shakespeare',\n",
       " 589: 'give',\n",
       " 590: 'telephone',\n",
       " 591: 'hand',\n",
       " 592: 'child',\n",
       " 593: 'government',\n",
       " 594: 'rights',\n",
       " 595: 'characters',\n",
       " 596: 'museum',\n",
       " 597: 'chicago',\n",
       " 598: 'asian',\n",
       " 599: 'hands',\n",
       " 600: 'numbers',\n",
       " 601: 'run',\n",
       " 602: 'nuclear',\n",
       " 603: 'also',\n",
       " 604: 'languages',\n",
       " 605: 'march',\n",
       " 606: '6',\n",
       " 607: 'shea',\n",
       " 608: 'gould',\n",
       " 609: 'middle',\n",
       " 610: 'massachusetts',\n",
       " 611: 'henry',\n",
       " 612: 'captain',\n",
       " 613: 'plays',\n",
       " 614: 'god',\n",
       " 615: 'o',\n",
       " 616: 'birds',\n",
       " 617: 'side',\n",
       " 618: 'eggs',\n",
       " 619: 'orange',\n",
       " 620: 'van',\n",
       " 621: 'thing',\n",
       " 622: 'son',\n",
       " 623: 'magazine',\n",
       " 624: 'sound',\n",
       " 625: 'greatest',\n",
       " 626: 'presidents',\n",
       " 627: 'german',\n",
       " 628: 'month',\n",
       " 629: 'days',\n",
       " 630: 'featured',\n",
       " 631: 'considered',\n",
       " 632: 'nine',\n",
       " 633: 'believe',\n",
       " 634: 'cowboy',\n",
       " 635: 'rain',\n",
       " 636: 'headquarters',\n",
       " 637: 'small',\n",
       " 638: 'rule',\n",
       " 639: 'products',\n",
       " 640: 'party',\n",
       " 641: 'oscar',\n",
       " 642: 'put',\n",
       " 643: 'cover',\n",
       " 644: 'thatcher',\n",
       " 645: 'france',\n",
       " 646: 'build',\n",
       " 647: 'winter',\n",
       " 648: 'southern',\n",
       " 649: 'titanic',\n",
       " 650: 'olympics',\n",
       " 651: 'security',\n",
       " 652: 'heart',\n",
       " 653: '8',\n",
       " 654: 'bond',\n",
       " 655: 'poet',\n",
       " 656: 'nobel',\n",
       " 657: 'but',\n",
       " 658: 'opera',\n",
       " 659: 'assassinated',\n",
       " 660: '-',\n",
       " 661: 'roman',\n",
       " 662: 'empire',\n",
       " 663: 'governor',\n",
       " 664: 'never',\n",
       " 665: 'titled',\n",
       " 666: 'comes',\n",
       " 667: 'painted',\n",
       " 668: 'golden',\n",
       " 669: 'inside',\n",
       " 670: 'artist',\n",
       " 671: 'wear',\n",
       " 672: 'thomas',\n",
       " 673: 'lakes',\n",
       " 674: 'stop',\n",
       " 675: 'nations',\n",
       " 676: 'because',\n",
       " 677: 'allowed',\n",
       " 678: 'paid',\n",
       " 679: 'cat',\n",
       " 680: 'students',\n",
       " 681: 'week',\n",
       " 682: 'stars',\n",
       " 683: 'ten',\n",
       " 684: 'jimmy',\n",
       " 685: 'claimed',\n",
       " 686: 'writer',\n",
       " 687: 'she',\n",
       " 688: 'spoken',\n",
       " 689: 'setting',\n",
       " 690: 'italy',\n",
       " 691: 'producer',\n",
       " 692: 'ireland',\n",
       " 693: 'castle',\n",
       " 694: 'living',\n",
       " 695: 'current',\n",
       " 696: 'elephant',\n",
       " 697: 'medical',\n",
       " 698: 'got',\n",
       " 699: 'balls',\n",
       " 700: 'near',\n",
       " 701: 'birthday',\n",
       " 702: 'square',\n",
       " 703: 'companies',\n",
       " 704: 'left',\n",
       " 705: 'face',\n",
       " 706: 'twins',\n",
       " 707: 'male',\n",
       " 708: 'court',\n",
       " 709: 'brand',\n",
       " 710: 'columbia',\n",
       " 711: 'program',\n",
       " 712: 'gave',\n",
       " 713: 'now',\n",
       " 714: 'set',\n",
       " 715: 'federal',\n",
       " 716: 'town',\n",
       " 717: 'nationality',\n",
       " 718: 'wars',\n",
       " 719: 'led',\n",
       " 720: 'mexico',\n",
       " 721: 'formed',\n",
       " 722: 'louis',\n",
       " 723: '11',\n",
       " 724: 'aids',\n",
       " 725: 'points',\n",
       " 726: 'caribbean',\n",
       " 727: 'ask',\n",
       " 728: 'corpus',\n",
       " 729: 'nnp',\n",
       " 730: 'bird',\n",
       " 731: 'lyrics',\n",
       " 732: 'books',\n",
       " 733: 'historical',\n",
       " 734: 'don',\n",
       " 735: 'spumante',\n",
       " 736: 'occur',\n",
       " 737: 'season',\n",
       " 738: 'mayor',\n",
       " 739: 'could',\n",
       " 740: 'police',\n",
       " 741: 'based',\n",
       " 742: 'commercial',\n",
       " 743: 'design',\n",
       " 744: 'cigarette',\n",
       " 745: 'schools',\n",
       " 746: 'ray',\n",
       " 747: 'hero',\n",
       " 748: 'came',\n",
       " 749: 'turn',\n",
       " 750: 'automobile',\n",
       " 751: 'remove',\n",
       " 752: 'followed',\n",
       " 753: 'buy',\n",
       " 754: 'online',\n",
       " 755: 'bone',\n",
       " 756: 'miss',\n",
       " 757: 'successful',\n",
       " 758: 'serve',\n",
       " 759: 'mountains',\n",
       " 760: 'yellow',\n",
       " 761: 'snow',\n",
       " 762: 'fire',\n",
       " 763: 'oil',\n",
       " 764: 'order',\n",
       " 765: '1899',\n",
       " 766: 'kid',\n",
       " 767: 'army',\n",
       " 768: 'wife',\n",
       " 769: 'e-mail',\n",
       " 770: 'milk',\n",
       " 771: 'video',\n",
       " 772: 'secretary',\n",
       " 773: 'machines',\n",
       " 774: 'sign',\n",
       " 775: 'stock',\n",
       " 776: 'given',\n",
       " 777: 'diego',\n",
       " 778: 'brown',\n",
       " 779: 'expression',\n",
       " 780: 'contact',\n",
       " 781: 'players',\n",
       " 782: 'india',\n",
       " 783: 'buffalo',\n",
       " 784: 'paint',\n",
       " 785: 'without',\n",
       " 786: 'early',\n",
       " 787: 'chicken',\n",
       " 788: 'italian',\n",
       " 789: 'written',\n",
       " 790: 'murder',\n",
       " 791: 'dead',\n",
       " 792: 'mozambique',\n",
       " 793: 'minimum',\n",
       " 794: 'wage',\n",
       " 795: '0',\n",
       " 796: 'cars',\n",
       " 797: 'atlantic',\n",
       " 798: 'glass',\n",
       " 799: 'comics',\n",
       " 800: 'least',\n",
       " 801: 'committee',\n",
       " 802: '1983',\n",
       " 803: 'career',\n",
       " 804: 'create',\n",
       " 805: 'celebrated',\n",
       " 806: 'sleep',\n",
       " 807: 'race',\n",
       " 808: 'mark',\n",
       " 809: 'same',\n",
       " 810: 'awarded',\n",
       " 811: 'amount',\n",
       " 812: 'cup',\n",
       " 813: 'weigh',\n",
       " 814: 'brain',\n",
       " 815: 'society',\n",
       " 816: 'flower',\n",
       " 817: 'natural',\n",
       " 818: 'silver',\n",
       " 819: 'cream',\n",
       " 820: 'developed',\n",
       " 821: 'watch',\n",
       " 822: 'alphabet',\n",
       " 823: 'fish',\n",
       " 824: '1963',\n",
       " 825: 'tokyo',\n",
       " 826: 'aaron',\n",
       " 827: 'korea',\n",
       " 828: 'vatican',\n",
       " 829: 'making',\n",
       " 830: 'follow',\n",
       " 831: 'nfl',\n",
       " 832: 'salt',\n",
       " 833: 'spain',\n",
       " 834: 'affect',\n",
       " 835: 'florida',\n",
       " 836: 'friend',\n",
       " 837: 'trial',\n",
       " 838: 'transplant',\n",
       " 839: 'lost',\n",
       " 840: 'originally',\n",
       " 841: 'effect',\n",
       " 842: 'per',\n",
       " 843: 'richest',\n",
       " 844: 'leave',\n",
       " 845: 'films',\n",
       " 846: 'silly',\n",
       " 847: 'course',\n",
       " 848: 'bay',\n",
       " 849: 'perfect',\n",
       " 850: 'bowling',\n",
       " 851: 'free',\n",
       " 852: 'inches',\n",
       " 853: 'inspired',\n",
       " 854: 'johnny',\n",
       " 855: 'reims',\n",
       " 856: 'page',\n",
       " 857: 'painting',\n",
       " 858: 'off',\n",
       " 859: 'read',\n",
       " 860: 'complete',\n",
       " 861: 'church',\n",
       " 862: 'jude',\n",
       " 863: 'elements',\n",
       " 864: 'received',\n",
       " 865: 'broadway',\n",
       " 866: 'produces',\n",
       " 867: 'border',\n",
       " 868: 'album',\n",
       " 869: 'poem',\n",
       " 870: 'grow',\n",
       " 871: 'maurizio',\n",
       " 872: 'pellegrin',\n",
       " 873: 'cocaine',\n",
       " 874: 'forest',\n",
       " 875: 'pole',\n",
       " 876: 'taste',\n",
       " 877: 'education',\n",
       " 878: 'watergate',\n",
       " 879: 'daily',\n",
       " 880: 'against',\n",
       " 881: 'composer',\n",
       " 882: 'swimming',\n",
       " 883: 'dc',\n",
       " 884: 'vegas',\n",
       " 885: 'microsoft',\n",
       " 886: '1994',\n",
       " 887: 'ways',\n",
       " 888: 'official',\n",
       " 889: 'spielberg',\n",
       " 890: '7',\n",
       " 891: 'done',\n",
       " 892: 'adult',\n",
       " 893: 'answers.com',\n",
       " 894: 'leading',\n",
       " 895: 'dollar',\n",
       " 896: 'pounds',\n",
       " 897: 'cancer',\n",
       " 898: '1991',\n",
       " 899: 'christian',\n",
       " 900: 'jewish',\n",
       " 901: 'declared',\n",
       " 902: 'chocolate',\n",
       " 903: 'equal',\n",
       " 904: 'represented',\n",
       " 905: 'lee',\n",
       " 906: 'emperor',\n",
       " 907: 'energy',\n",
       " 908: 'correct',\n",
       " 909: 'must',\n",
       " 910: 'model',\n",
       " 911: 'jane',\n",
       " 912: 'any',\n",
       " 913: 'charlie',\n",
       " 914: 'pop',\n",
       " 915: 'appeared',\n",
       " 916: 'tale',\n",
       " 917: 'gate',\n",
       " 918: 'kept',\n",
       " 919: 'degrees',\n",
       " 920: 'meant',\n",
       " 921: 'host',\n",
       " 922: 'always',\n",
       " 923: 'liberty',\n",
       " 924: 'mccarren',\n",
       " 925: 'magic',\n",
       " 926: 'presidential',\n",
       " 927: 'file',\n",
       " 928: 'members',\n",
       " 929: 'drive',\n",
       " 930: 'turned',\n",
       " 931: 'contract',\n",
       " 932: 'site',\n",
       " 933: 'going',\n",
       " 934: 'electric',\n",
       " 935: 'stone',\n",
       " 936: 'ord',\n",
       " 937: 'bureau',\n",
       " 938: 'investigation',\n",
       " 939: 'johnson',\n",
       " 940: 'phone',\n",
       " 941: 'daughter',\n",
       " 942: 'lincoln',\n",
       " 943: 'right',\n",
       " 944: 'gulf',\n",
       " 945: 'literary',\n",
       " 946: 'bottle',\n",
       " 947: 'told',\n",
       " 948: 'houses',\n",
       " 949: 'birthstone',\n",
       " 950: 'sold',\n",
       " 951: 'reach',\n",
       " 952: 'alley',\n",
       " 953: 'events',\n",
       " 954: 'francisco',\n",
       " 955: 'madonna',\n",
       " 956: 'native',\n",
       " 957: 'medicine',\n",
       " 958: 'himself',\n",
       " 959: 'voice',\n",
       " 960: 'project',\n",
       " 961: 'mercury',\n",
       " 962: 'caffeine',\n",
       " 963: 'took',\n",
       " 964: 'using',\n",
       " 965: 'owns',\n",
       " 966: 'vhs',\n",
       " 967: 'research',\n",
       " 968: 'large',\n",
       " 969: 'michael',\n",
       " 970: 'drug',\n",
       " 971: 'britain',\n",
       " 972: 'invent',\n",
       " 973: 'treat',\n",
       " 974: 'mr.',\n",
       " 975: 'mary',\n",
       " 976: 'contain',\n",
       " 977: 'jersey',\n",
       " 978: 'conference',\n",
       " 979: 'justice',\n",
       " 980: 'd.',\n",
       " 981: 'd.c.',\n",
       " 982: 'simpsons',\n",
       " 983: 'close',\n",
       " 984: 'having',\n",
       " 985: 'location',\n",
       " 986: 'usa',\n",
       " 987: 'muppets',\n",
       " 988: 'rocky',\n",
       " 989: 'hockey',\n",
       " 990: 'our',\n",
       " 991: 'yankee',\n",
       " 992: 'franklin',\n",
       " 993: 'roosevelt',\n",
       " 994: 'sex',\n",
       " 995: 'animated',\n",
       " 996: 'lady',\n",
       " 997: 'diamond',\n",
       " 998: 'single',\n",
       " 999: 'asia',\n",
       " ...}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverse_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Catherine - Current\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:44: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-b4c5b1ec9d7c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[0msample_batch_inputs_2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msample_batch_labels_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msample_gen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m assert np.all(np.asarray([dictionary[w] for w in train_questions[0]],dtype=np.int32) \n\u001b[1;32m---> 44\u001b[1;33m               == np.argmax(sample_batch_inputs[0,:,:],axis=1))\n\u001b[0m\u001b[0;32m     45\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Sample batch labels'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_batch_labels\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "sent_length = 40\n",
    "num_classes = 6\n",
    "all_labels = ['NUM','LOC','HUM','DESC','ENTY','ABBR'] # All the types of question that are in the dataset\n",
    "\n",
    "class BatchGenerator(object):\n",
    "    def __init__(self,batch_size,questions,labels):\n",
    "        self.questions = questions\n",
    "        self.labels = labels\n",
    "        self.text_size = len(questions)\n",
    "        self.batch_size = batch_size\n",
    "        self.data_index = 0\n",
    "        assert len(self.questions)==len(self.labels)\n",
    "        \n",
    "    def generate_batch(self):\n",
    "        global sent_length,num_classes\n",
    "        global dictionary\n",
    "        inputs = np.zeros((self.batch_size,sent_length,vocabulary_size),dtype=np.float32)\n",
    "        labels_ohe = np.zeros((self.batch_size,num_classes),dtype=np.float32)\n",
    "        \n",
    "        if self.data_index + self.batch_size >= self.text_size:\n",
    "            self.data_index = 0\n",
    "            \n",
    "        for qi,que in enumerate(self.questions[self.data_index:self.data_index+self.batch_size]):\n",
    "            for wi,word in enumerate(que):\n",
    "                #q_arr[qi*batch_size + wi] = dictionary[word]\n",
    "                inputs[qi,wi,dictionary[word]] = 1.0\n",
    "            \n",
    "            #print(self.data_index,qi,len(self.labels))\n",
    "            labels_ohe[qi,all_labels.index(self.labels[self.data_index + qi])] = 1.0\n",
    "            #print('%s'%([w for w in que]),self.labels[self.data_index+qi])            \n",
    "            \n",
    "        self.data_index = (self.data_index + self.batch_size)%self.text_size\n",
    "            \n",
    "        return inputs,labels_ohe\n",
    "    \n",
    "    def return_index(self):\n",
    "        return self.data_index\n",
    "    \n",
    "sample_gen = BatchGenerator(batch_size,train_questions,train_labels)\n",
    "sample_batch_inputs,sample_batch_labels = sample_gen.generate_batch()\n",
    "sample_batch_inputs_2,sample_batch_labels_2 = sample_gen.generate_batch()\n",
    "assert np.all(np.asarray([dictionary[w] for w in train_questions[0]],dtype=np.int32) \n",
    "              == np.argmax(sample_batch_inputs[0,:,:],axis=1))\n",
    "print('Sample batch labels')\n",
    "print(np.argmax(sample_batch_labels,axis=1))\n",
    "print(np.argmax(sample_batch_labels_2,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "\n",
    "# inputs and labels\n",
    "sent_inputs = tf.placeholder(shape=[batch_size,sent_length,vocabulary_size],dtype=tf.float32,name='sentence_inputs')\n",
    "sent_labels = tf.placeholder(shape=[batch_size,num_classes],dtype=tf.float32,name='sentence_labels')\n",
    "\n",
    "# 3 filters with different context window sizes (3,5,7)\n",
    "# Each of this filter spans the full one-hot-encoded length of each word and the context window width\n",
    "w1 = tf.Variable(tf.truncated_normal([3,vocabulary_size,1],stddev=0.02,dtype=tf.float32),name='weights_1')\n",
    "b1 = tf.Variable(tf.random_uniform([1],0,0.01,dtype=tf.float32),name='bias_1')\n",
    "\n",
    "w2 = tf.Variable(tf.truncated_normal([5,vocabulary_size,1],stddev=0.02,dtype=tf.float32),name='weights_2')\n",
    "b2 = tf.Variable(tf.random_uniform([1],0,0.01,dtype=tf.float32),name='bias_2')\n",
    "\n",
    "w3 = tf.Variable(tf.truncated_normal([7,vocabulary_size,1],stddev=0.02,dtype=tf.float32),name='weights_3')\n",
    "b3 = tf.Variable(tf.random_uniform([1],0,0.01,dtype=tf.float32),name='bias_3')\n",
    "\n",
    "# Calculate the output for all the filters with a stride 1\n",
    "h1_1 = tf.nn.tanh(tf.nn.conv1d(sent_inputs,w1,stride=1,padding='SAME') + b1)\n",
    "h1_2 = tf.nn.tanh(tf.nn.conv1d(sent_inputs,w2,stride=1,padding='SAME') + b2)\n",
    "h1_3 = tf.nn.tanh(tf.nn.conv1d(sent_inputs,w3,stride=1,padding='SAME') + b3)\n",
    "\n",
    "# This is doing the max pooling. Thereare two options to do the max pooling\n",
    "# 1. Use tf.nn.max_pool operation on a tensor made by concatenating h1_1,h1_2,h1_3 and converting that tensor to 4D\n",
    "# (Because max_pool takes a tensor of rank >= 4 )\n",
    "# 2. Do the max pooling separately for each filter output and combine them using tf.concat \n",
    "# (this is the one used in the code)\n",
    "\n",
    "h2_1 = tf.reduce_max(h1_1,axis=1)\n",
    "h2_2 = tf.reduce_max(h1_2,axis=1)\n",
    "h2_3 = tf.reduce_max(h1_3,axis=1)\n",
    "\n",
    "h2 = tf.concat([h2_1,h2_2,h2_3],axis=1)\n",
    "h2_shape = h2.get_shape().as_list()\n",
    "\n",
    "# Weights and bias of the output layer\n",
    "w_fc1 = tf.Variable(tf.truncated_normal([h2_shape[1],num_classes],stddev=0.005,dtype=tf.float32),name='weights_fulcon_1')\n",
    "b_fc1 = tf.Variable(tf.random_uniform([num_classes],0,0.01,dtype=tf.float32),name='bias_fulcon_1')\n",
    "\n",
    "# since h2 is 2d [batch_size,output_width] reshaping the output is not required as it usually do in CNNs\n",
    "logits = tf.matmul(h2,w_fc1) + b_fc1\n",
    "\n",
    "predictions = tf.argmax(tf.nn.softmax(logits),axis=1)\n",
    "\n",
    "# Loss (Cross-Entropy)\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=sent_labels,logits=logits))\n",
    "\n",
    "# Momentum Optimizer\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate=0.01,momentum=0.9).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "\n",
      "Train Loss at Epoch 0: 1.68\n",
      "Test accuracy at Epoch 0: 13.105\n",
      "Train Loss at Epoch 1: 1.57\n",
      "Test accuracy at Epoch 1: 43.952\n",
      "Train Loss at Epoch 2: 0.92\n",
      "Test accuracy at Epoch 2: 75.202\n",
      "Train Loss at Epoch 3: 0.60\n",
      "Test accuracy at Epoch 3: 79.032\n",
      "Train Loss at Epoch 4: 0.45\n",
      "Test accuracy at Epoch 4: 83.065\n",
      "Train Loss at Epoch 5: 0.33\n",
      "Test accuracy at Epoch 5: 92.540\n",
      "Train Loss at Epoch 6: 0.22\n",
      "Test accuracy at Epoch 6: 93.750\n",
      "Train Loss at Epoch 7: 0.16\n",
      "Test accuracy at Epoch 7: 94.153\n",
      "Train Loss at Epoch 8: 0.13\n",
      "Test accuracy at Epoch 8: 93.952\n",
      "Train Loss at Epoch 9: 0.11\n",
      "Test accuracy at Epoch 9: 93.952\n",
      "Train Loss at Epoch 10: 0.10\n",
      "Test accuracy at Epoch 10: 95.363\n",
      "Train Loss at Epoch 11: 0.09\n",
      "Test accuracy at Epoch 11: 95.968\n",
      "Train Loss at Epoch 12: 0.08\n",
      "Test accuracy at Epoch 12: 95.968\n",
      "Train Loss at Epoch 13: 0.07\n",
      "Test accuracy at Epoch 13: 95.968\n",
      "Train Loss at Epoch 14: 0.07\n",
      "Test accuracy at Epoch 14: 95.968\n",
      "Train Loss at Epoch 15: 0.06\n",
      "Test accuracy at Epoch 15: 96.169\n",
      "Train Loss at Epoch 16: 0.06\n",
      "Test accuracy at Epoch 16: 96.169\n",
      "Train Loss at Epoch 17: 0.05\n",
      "Test accuracy at Epoch 17: 96.169\n",
      "Train Loss at Epoch 18: 0.05\n",
      "Test accuracy at Epoch 18: 96.371\n",
      "Train Loss at Epoch 19: 0.05\n",
      "Test accuracy at Epoch 19: 96.371\n",
      "Train Loss at Epoch 20: 0.04\n",
      "Test accuracy at Epoch 20: 96.371\n",
      "Train Loss at Epoch 21: 0.04\n",
      "Test accuracy at Epoch 21: 96.169\n",
      "Train Loss at Epoch 22: 0.04\n",
      "Test accuracy at Epoch 22: 96.169\n",
      "Train Loss at Epoch 23: 0.04\n",
      "Test accuracy at Epoch 23: 96.169\n",
      "Train Loss at Epoch 24: 0.04\n",
      "Test accuracy at Epoch 24: 96.169\n",
      "Train Loss at Epoch 25: 0.04\n",
      "Test accuracy at Epoch 25: 95.968\n",
      "Train Loss at Epoch 26: 0.03\n",
      "Test accuracy at Epoch 26: 96.169\n",
      "Train Loss at Epoch 27: 0.03\n",
      "Test accuracy at Epoch 27: 95.968\n",
      "Train Loss at Epoch 28: 0.03\n",
      "Test accuracy at Epoch 28: 96.169\n",
      "Train Loss at Epoch 29: 0.03\n",
      "Test accuracy at Epoch 29: 96.976\n",
      "Train Loss at Epoch 30: 0.03\n",
      "Test accuracy at Epoch 30: 96.976\n",
      "Train Loss at Epoch 31: 0.03\n",
      "Test accuracy at Epoch 31: 96.976\n",
      "Train Loss at Epoch 32: 0.03\n",
      "Test accuracy at Epoch 32: 96.976\n",
      "Train Loss at Epoch 33: 0.03\n",
      "Test accuracy at Epoch 33: 96.976\n",
      "Train Loss at Epoch 34: 0.02\n",
      "Test accuracy at Epoch 34: 96.976\n",
      "Train Loss at Epoch 35: 0.02\n",
      "Test accuracy at Epoch 35: 96.976\n",
      "Train Loss at Epoch 36: 0.02\n",
      "Test accuracy at Epoch 36: 96.976\n",
      "Train Loss at Epoch 37: 0.02\n",
      "Test accuracy at Epoch 37: 96.976\n",
      "Train Loss at Epoch 38: 0.02\n",
      "Test accuracy at Epoch 38: 96.976\n",
      "Train Loss at Epoch 39: 0.02\n",
      "Test accuracy at Epoch 39: 96.976\n",
      "Train Loss at Epoch 40: 0.02\n",
      "Test accuracy at Epoch 40: 96.976\n",
      "Train Loss at Epoch 41: 0.02\n",
      "Test accuracy at Epoch 41: 96.774\n",
      "Train Loss at Epoch 42: 0.02\n",
      "Test accuracy at Epoch 42: 96.774\n",
      "Train Loss at Epoch 43: 0.02\n",
      "Test accuracy at Epoch 43: 96.774\n",
      "Train Loss at Epoch 44: 0.02\n",
      "Test accuracy at Epoch 44: 96.774\n",
      "Train Loss at Epoch 45: 0.02\n",
      "Test accuracy at Epoch 45: 96.976\n",
      "Train Loss at Epoch 46: 0.02\n",
      "Test accuracy at Epoch 46: 96.976\n",
      "Train Loss at Epoch 47: 0.02\n",
      "Test accuracy at Epoch 47: 96.976\n",
      "Train Loss at Epoch 48: 0.02\n",
      "Test accuracy at Epoch 48: 96.976\n",
      "Train Loss at Epoch 49: 0.02\n",
      "Test accuracy at Epoch 49: 96.976\n"
     ]
    }
   ],
   "source": [
    "# With filter widths [3,5,7] the algorithm achieves around ~90% accuracy on test dataset (50 epochs). \n",
    "\n",
    "session = tf.InteractiveSession()\n",
    "\n",
    "batch_size = 16\n",
    "num_steps = 50\n",
    "tf.global_variables_initializer().run()\n",
    "train_gen = BatchGenerator(batch_size,train_questions,train_labels)\n",
    "test_gen = BatchGenerator(batch_size,test_questions,test_labels)\n",
    "\n",
    "test_interval = 1\n",
    "\n",
    "# Get accuracy \n",
    "def accuracy(labels,preds):\n",
    "    return np.sum(np.argmax(labels,axis=1)==preds)/labels.shape[0]\n",
    "\n",
    "print('Initialized\\n')\n",
    "\n",
    "\n",
    "for step in range(num_steps):\n",
    "    avg_loss = []\n",
    "    for tr_i in range((len(train_questions)//batch_size)-1):\n",
    "        tr_inputs, tr_labels = train_gen.generate_batch()\n",
    "        \n",
    "        l,_ = session.run([loss,optimizer],feed_dict={sent_inputs: tr_inputs, sent_labels: tr_labels})\n",
    "        avg_loss.append(l)\n",
    "        \n",
    "    print('Train Loss at Epoch %d: %.2f'%(step,np.mean(avg_loss)))\n",
    "    test_accuracy = []\n",
    "    if (step+1)%test_interval==0:        \n",
    "        for ts_i in range((len(test_questions)-1)//batch_size):\n",
    "            ts_inputs,ts_labels = test_gen.generate_batch()\n",
    "            preds = session.run(predictions,feed_dict={sent_inputs: ts_inputs, sent_labels: ts_labels})\n",
    "            test_accuracy.append(accuracy(ts_labels,preds))\n",
    "            \n",
    "        print('Test accuracy at Epoch %d: %.3f'%(step,np.mean(test_accuracy)*100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
